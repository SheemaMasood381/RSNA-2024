{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSNA Lumbar Spine Degenerative Classification - Inference with Pre-trained CNN Model\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Lumbar spine degeneration is a common condition affecting a large portion of the global population, often leading to pain and mobility issues. Accurate classification and diagnosis of this condition through imaging, particularly MRI and X-ray scans, is crucial for timely and effective treatment. In this notebook, we will perform inference using a pre-trained Convolutional Neural Network (CNN) model to classify lumbar spine degeneration based on imaging data.\n",
    "\n",
    "Our model has been pre-trained on the **RSNA 2024 Lumbar Spine Dataset**, which includes labeled X-ray images of patients with varying levels of degenerative disease. The goal is to utilize the trained model to make predictions on new, unseen images, classifying the level of degeneration according to standard medical categories.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The main objective of this notebook is to load the pre-trained CNN model and use it for inference on new test data. We will perform the following steps:\n",
    "\n",
    "1. **Load the pre-trained model:** We'll load the saved CNN model that was trained to classify lumbar spine degeneration.\n",
    "2. **Preprocess new images:** New test images will be preprocessed to match the input format expected by the model.\n",
    "3. **Make predictions:** The model will be used to predict the level of degeneration for each image.\n",
    "4. **Interpret the results:** We will interpret the model's output and map predictions to the corresponding categories of degeneration.\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "The RSNA 2024 dataset consists of labeled lumbar spine X-ray images categorized based on different grades of degeneration. This classification helps healthcare professionals assess the severity of degeneration, which aids in treatment planning.\n",
    "\n",
    "## Pre-trained Model\n",
    "\n",
    "The CNN model was trained using transfer learning, leveraging a state-of-the-art architecture that has been fine-tuned on the RSNA dataset for better performance on spine degeneration classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:12.673156Z",
     "iopub.status.busy": "2024-10-06T14:58:12.672678Z",
     "iopub.status.idle": "2024-10-06T14:58:13.370989Z",
     "shell.execute_reply": "2024-10-06T14:58:13.369700Z",
     "shell.execute_reply.started": "2024-10-06T14:58:12.673107Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pydicom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "In this section, we define the path to the dataset and load the test series descriptions, which will help us understand the data we'll be working with.\n",
    "\n",
    "1. **Dataset Path:**\n",
    "   - We define `train_path`, which points to the location of the dataset in a Kaggle directory. This path is used to load various files related to lumbar spine degeneration classification.\n",
    "   \n",
    "2. **Loading Test Series Descriptions:**\n",
    "   - We use the `pd.read_csv()` function to load a CSV file named `test_series_descriptions.csv`. This file contains important information about the test data series that we will use for inference.\n",
    "\n",
    "3. **Dataset Overview:**\n",
    "   - The `info()` method is used to display a summary of the DataFrame, which includes the total number of entries, column names, data types, and the count of non-null values. This provides a quick understanding of the dataset's structure and whether there are any missing values.\n",
    "   \n",
    "By loading this information, we ensure we have a clear view of the test dataset before proceeding with further analysis and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:14.702891Z",
     "iopub.status.busy": "2024-10-06T14:58:14.702176Z",
     "iopub.status.idle": "2024-10-06T14:58:14.722213Z",
     "shell.execute_reply": "2024-10-06T14:58:14.720846Z",
     "shell.execute_reply.started": "2024-10-06T14:58:14.702847Z"
    }
   },
   "outputs": [],
   "source": [
    "# the path to the dataset\n",
    "\n",
    "train_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/'\n",
    "\n",
    "test_description = pd.read_csv(train_path + 'test_series_descriptions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:15.518332Z",
     "iopub.status.busy": "2024-10-06T14:58:15.517902Z",
     "iopub.status.idle": "2024-10-06T14:58:15.553489Z",
     "shell.execute_reply": "2024-10-06T14:58:15.552141Z",
     "shell.execute_reply.started": "2024-10-06T14:58:15.518288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   study_id            3 non-null      int64 \n",
      " 1   series_id           3 non-null      int64 \n",
      " 2   series_description  3 non-null      object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 200.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "test_description.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Image Paths\n",
    "\n",
    "In this section, we define a function that helps us generate the paths to the test images based on the datasetâ€™s directory structure. This is crucial as the images are stored in a nested folder format.\n",
    "\n",
    "1. **Importing Necessary Libraries:**\n",
    "   - We import the `os` library to handle directory operations and the `cv2` (OpenCV) library for image processing tasks. Although OpenCV is imported here, it might be used later for reading or manipulating images.\n",
    "\n",
    "2. **`generate_image_paths()` Function:**\n",
    "   - This function generates the full paths to the images based on the directory structure. Each image is stored in a nested directory where the folders represent different `study_id` and `series_id` combinations.\n",
    "   \n",
    "   - **Arguments:**\n",
    "     - `df`: A DataFrame that contains columns `study_id` and `series_id`, which correspond to unique identifiers for studies and series in the dataset.\n",
    "     - `data_dir`: The base directory where the images are stored.\n",
    "     \n",
    "   - **Process:**\n",
    "     - For each row in the DataFrame, the function constructs the path to the folder containing the images using `study_id` and `series_id`. \n",
    "     - It then lists all the files (images) in the directory and generates the full path for each image by combining the folder path with the image filename.\n",
    "     - These image paths are collected in a list, `image_paths`, which is returned at the end.\n",
    "\n",
    "3. **Applying the Function:**\n",
    "   - The function is applied to the `test_description` DataFrame using the test image directory path (`f'{train_path}/test_images'`). This results in a list of full paths to all test images, which is stored in `test_image_paths`.\n",
    "\n",
    "By generating the full image paths, we prepare the data for later stages such as image loading and preprocessing, which are necessary before performing inference with the CNN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:16.825777Z",
     "iopub.status.busy": "2024-10-06T14:58:16.825346Z",
     "iopub.status.idle": "2024-10-06T14:58:17.088575Z",
     "shell.execute_reply": "2024-10-06T14:58:17.087125Z",
     "shell.execute_reply.started": "2024-10-06T14:58:16.825713Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to generate image paths based on directory structure\n",
    "def generate_image_paths(df, data_dir):\n",
    "    image_paths = []\n",
    "    for study_id, series_id in zip(df['study_id'], df['series_id']):\n",
    "        study_dir = os.path.join(data_dir, str(study_id))\n",
    "        series_dir = os.path.join(study_dir, str(series_id))\n",
    "        images = os.listdir(series_dir)\n",
    "        image_paths.extend([os.path.join(series_dir, img) for img in images])\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "test_image_paths = generate_image_paths(test_description, f'{train_path}/test_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition Mapping\n",
    "\n",
    "This section defines a dictionary, `condition_mapping`, which maps different types of medical image modalities to specific conditions related to lumbar spine degeneration.\n",
    "\n",
    "1. **Purpose:**\n",
    "   - The `condition_mapping` dictionary is used to associate specific image views (such as Sagittal and Axial) with conditions affecting the lumbar spine. These mappings will help us interpret the model's output and classify the type of spine degeneration based on which view of the spine is being analyzed.\n",
    "\n",
    "2. **Key-Value Pairs:**\n",
    "   - Each key in the dictionary refers to a specific type of medical image modality:\n",
    "     - **Sagittal T1:** This refers to images in the sagittal plane (side view) taken using T1-weighted MRI scans. The dictionary specifies two conditions, one for the left side and one for the right side:\n",
    "       - `left_neural_foraminal_narrowing`: Narrowing of the neural foramen on the left side.\n",
    "       - `right_neural_foraminal_narrowing`: Narrowing of the neural foramen on the right side.\n",
    "   \n",
    "     - **Axial T2:** This refers to images in the axial plane (cross-sectional view) using T2-weighted MRI scans. The conditions mapped are:\n",
    "       - `left_subarticular_stenosis`: Subarticular stenosis (narrowing) on the left side.\n",
    "       - `right_subarticular_stenosis`: Subarticular stenosis on the right side.\n",
    "   \n",
    "     - **Sagittal T2/STIR:** This corresponds to sagittal images using T2-weighted or STIR (Short-TI Inversion Recovery) techniques. The mapped condition is:\n",
    "       - `spinal_canal_stenosis`: Narrowing of the spinal canal.\n",
    "\n",
    "3. **Use Case:**\n",
    "   - This mapping is important because different image types highlight different anatomical structures and conditions. For example, sagittal views are more useful for assessing neural foraminal narrowing, while axial views can show subarticular stenosis. By using this dictionary, we can automatically determine which condition is related to the image type being processed.\n",
    "\n",
    "This mapping provides a structured way to interpret the model's predictions based on the specific MRI sequence used for each image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:18.379459Z",
     "iopub.status.busy": "2024-10-06T14:58:18.379021Z",
     "iopub.status.idle": "2024-10-06T14:58:18.385387Z",
     "shell.execute_reply": "2024-10-06T14:58:18.384007Z",
     "shell.execute_reply.started": "2024-10-06T14:58:18.379418Z"
    }
   },
   "outputs": [],
   "source": [
    "condition_mapping = {\n",
    "    'Sagittal T1': {'left': 'left_neural_foraminal_narrowing', 'right': 'right_neural_foraminal_narrowing'},\n",
    "    'Axial T2': {'left': 'left_subarticular_stenosis', 'right': 'right_subarticular_stenosis'},\n",
    "    'Sagittal T2/STIR': 'spinal_canal_stenosis'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding the Dataset with Image Paths and Conditions\n",
    "\n",
    "In this section, we are expanding the test dataset by associating each image with its corresponding condition(s) based on the series description. The result is a new DataFrame (`test_df`) that contains additional details about each image.\n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "1. **`get_image_paths(row)` Function:**\n",
    "   - This function generates the file paths for all images associated with a particular row (study and series) in the DataFrame.\n",
    "   - **Parameters:**\n",
    "     - `row`: A row from the `test_description` DataFrame containing information such as `study_id` and `series_id`.\n",
    "   - **Process:**\n",
    "     - The function constructs the path to the series folder by joining `base_path` with the study and series IDs.\n",
    "     - If the folder exists, it lists all files in that directory and returns their full paths.\n",
    "     - If the folder doesn't exist, it returns an empty list.\n",
    "\n",
    "2. **Expanding the Rows:**\n",
    "   - The `expanded_rows` list will store the expanded data for each image and its corresponding condition(s).\n",
    "   - We iterate over each row in the `test_description` DataFrame using `iterrows()` to access the details of each study and series.\n",
    "\n",
    "3. **Mapping Conditions to Series Descriptions:**\n",
    "   - For each row, we use `condition_mapping` to retrieve the relevant conditions based on the `series_description`. If a single condition is returned (e.g., for `Sagittal T2/STIR`), it is converted into a dictionary that applies the condition to both the left and right sides.\n",
    "   - If multiple conditions (e.g., left and right for neural foraminal narrowing) are returned, they are stored in a dictionary format.\n",
    "\n",
    "4. **Appending Expanded Data:**\n",
    "   - For each condition (left or right) and image path generated by `get_image_paths()`, we create a dictionary with the following fields:\n",
    "     - `study_id`: The unique identifier for the study.\n",
    "     - `series_id`: The identifier for the imaging series.\n",
    "     - `series_description`: A description of the imaging series (e.g., Sagittal T1, Axial T2).\n",
    "     - `image_path`: The full path to the image file.\n",
    "     - `condition`: The condition being assessed (e.g., left neural foraminal narrowing, spinal canal stenosis).\n",
    "     - `row_id`: A unique identifier for the row, constructed by concatenating the `study_id` and the condition.\n",
    "\n",
    "   - This dictionary is appended to the `expanded_rows` list, essentially \"flattening\" the relationship between series, images, and conditions.\n",
    "\n",
    "5. **Creating the Final DataFrame (`test_df`):**\n",
    "   - Finally, the expanded data stored in `expanded_rows` is converted into a new DataFrame (`test_df`). This DataFrame contains additional rows, with each row representing a unique image and its corresponding condition.\n",
    "\n",
    "#### Purpose:\n",
    "This approach expands the test dataset so that each individual image is associated with the correct condition(s) based on the type of imaging series it belongs to. This structure is crucial for making accurate predictions during inference, as each image is linked with the medical condition that the model needs to classify.\n",
    "\n",
    "By organizing the data in this way, we ensure that the CNN model can be applied effectively to each test image and its corresponding medical condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:19.834528Z",
     "iopub.status.busy": "2024-10-06T14:58:19.834071Z",
     "iopub.status.idle": "2024-10-06T14:58:19.839971Z",
     "shell.execute_reply": "2024-10-06T14:58:19.838662Z",
     "shell.execute_reply.started": "2024-10-06T14:58:19.834477Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:20.356966Z",
     "iopub.status.busy": "2024-10-06T14:58:20.356535Z",
     "iopub.status.idle": "2024-10-06T14:58:20.364503Z",
     "shell.execute_reply": "2024-10-06T14:58:20.363277Z",
     "shell.execute_reply.started": "2024-10-06T14:58:20.356925Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_image_paths(row):\n",
    "    series_path = os.path.join(base_path, str(row['study_id']), str(row['series_id']))\n",
    "    if os.path.exists(series_path):\n",
    "        return [os.path.join(series_path, f) for f in os.listdir(series_path) if os.path.isfile(os.path.join(series_path, f))]\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:20.893920Z",
     "iopub.status.busy": "2024-10-06T14:58:20.892902Z",
     "iopub.status.idle": "2024-10-06T14:58:20.988673Z",
     "shell.execute_reply": "2024-10-06T14:58:20.987426Z",
     "shell.execute_reply.started": "2024-10-06T14:58:20.893872Z"
    }
   },
   "outputs": [],
   "source": [
    "expanded_rows = []\n",
    "for index, row in test_description.iterrows():\n",
    "    image_paths = get_image_paths(row)\n",
    "    conditions = condition_mapping.get(row['series_description'], {})\n",
    "    if isinstance(conditions, str):  # Single condition\n",
    "        conditions = {'left': conditions, 'right': conditions}\n",
    "    for side, condition in conditions.items():\n",
    "        for image_path in image_paths:\n",
    "            expanded_rows.append({\n",
    "                'study_id': row['study_id'],\n",
    "                'series_id': row['series_id'],\n",
    "                'series_description': row['series_description'],\n",
    "                'image_path': image_path,\n",
    "                'condition': condition,\n",
    "                'row_id': f\"{row['study_id']}_{condition}\"\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:21.791665Z",
     "iopub.status.busy": "2024-10-06T14:58:21.791194Z",
     "iopub.status.idle": "2024-10-06T14:58:21.798291Z",
     "shell.execute_reply": "2024-10-06T14:58:21.797072Z",
     "shell.execute_reply.started": "2024-10-06T14:58:21.791621Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(expanded_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Levels to the `row_id`\n",
    "\n",
    "In this section, we modify the `row_id` to incorporate specific spinal levels (e.g., L1-L2, L2-L3) into the identifier. This helps differentiate images based on the anatomical region of the spine being assessed.\n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "1. **Defining Levels:**\n",
    "   - We define a list called `levels` that represents various spinal levels:\n",
    "     - `'l1_l2'`, `'l2_l3'`, `'l3_l4'`, `'l4_l5'`, and `'l5_s1'`.\n",
    "   - These levels correspond to different regions of the lumbar spine where degeneration is typically assessed.\n",
    "\n",
    "2. **`update_row_id()` Function:**\n",
    "   - This function updates the `row_id` by appending one of the spinal levels from the `levels` list.\n",
    "   - **Parameters:**\n",
    "     - `row`: A row from the `test_df` DataFrame.\n",
    "     - `levels`: The list of spinal levels that will be cycled through for each row.\n",
    "   - **Process:**\n",
    "     - The function calculates the appropriate level for the current row using `row.name % len(levels)`. The modulus operator ensures that the function cycles through the levels repeatedly, so each row is assigned one of the spinal levels in sequence.\n",
    "     - The new `row_id` is constructed by concatenating the `study_id`, `condition`, and the assigned level.\n",
    "   \n",
    "3. **Applying the `update_row_id()` Function:**\n",
    "   - We use the `apply()` method to apply the `update_row_id()` function to each row in `test_df`. This updates the `row_id` column to include the spinal level.\n",
    "   - The lambda function ensures that each row in `test_df` is passed to `update_row_id()`, and the updated `row_id` is returned for each row.\n",
    "\n",
    "4. **Viewing the Updated DataFrame:**\n",
    "   - After updating the `row_id`, we display the first few rows of the updated DataFrame using `head()`. This allows us to verify that the `row_id` now includes the correct level along with the `study_id` and `condition`.\n",
    "\n",
    "#### Purpose:\n",
    "By incorporating spinal levels into the `row_id`, we ensure that each image and its corresponding condition are uniquely identified, not just by the study and condition but also by the anatomical region being assessed. This is particularly useful when the model needs to make predictions for different regions of the spine, as it allows for more granular and detailed analysis.\n",
    "\n",
    "This structured identification will help ensure that the model outputs are specific to the correct region of the lumbar spine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:22.978696Z",
     "iopub.status.busy": "2024-10-06T14:58:22.978244Z",
     "iopub.status.idle": "2024-10-06T14:58:23.010012Z",
     "shell.execute_reply": "2024-10-06T14:58:23.008861Z",
     "shell.execute_reply.started": "2024-10-06T14:58:22.978652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_description</th>\n",
       "      <th>image_path</th>\n",
       "      <th>condition</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l1_l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l2_l3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l3_l4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l4_l5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44036939</td>\n",
       "      <td>2828203845</td>\n",
       "      <td>Sagittal T1</td>\n",
       "      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n",
       "      <td>left_neural_foraminal_narrowing</td>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l5_s1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id   series_id series_description  \\\n",
       "0  44036939  2828203845        Sagittal T1   \n",
       "1  44036939  2828203845        Sagittal T1   \n",
       "2  44036939  2828203845        Sagittal T1   \n",
       "3  44036939  2828203845        Sagittal T1   \n",
       "4  44036939  2828203845        Sagittal T1   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  /kaggle/input/rsna-2024-lumbar-spine-degenerat...   \n",
       "1  /kaggle/input/rsna-2024-lumbar-spine-degenerat...   \n",
       "2  /kaggle/input/rsna-2024-lumbar-spine-degenerat...   \n",
       "3  /kaggle/input/rsna-2024-lumbar-spine-degenerat...   \n",
       "4  /kaggle/input/rsna-2024-lumbar-spine-degenerat...   \n",
       "\n",
       "                         condition  \\\n",
       "0  left_neural_foraminal_narrowing   \n",
       "1  left_neural_foraminal_narrowing   \n",
       "2  left_neural_foraminal_narrowing   \n",
       "3  left_neural_foraminal_narrowing   \n",
       "4  left_neural_foraminal_narrowing   \n",
       "\n",
       "                                           row_id  \n",
       "0  44036939_left_neural_foraminal_narrowing_l1_l2  \n",
       "1  44036939_left_neural_foraminal_narrowing_l2_l3  \n",
       "2  44036939_left_neural_foraminal_narrowing_l3_l4  \n",
       "3  44036939_left_neural_foraminal_narrowing_l4_l5  \n",
       "4  44036939_left_neural_foraminal_narrowing_l5_s1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Levels for row_id\n",
    "levels = ['l1_l2', 'l2_l3', 'l3_l4', 'l4_l5', 'l5_s1']\n",
    "\n",
    "# update row_id with levels\n",
    "def update_row_id(row, levels):\n",
    "    level = levels[row.name % len(levels)]  \n",
    "    return f\"{row['study_id']}_{row['condition']}_{level}\"\n",
    "\n",
    "# Update row_id in expanded_test_desc to include levels\n",
    "test_df['row_id'] = test_df.apply(lambda row: update_row_id(row, levels), axis=1)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset Class for Inference\n",
    "\n",
    "This section defines a custom class `TestDataset` that is used to handle the loading and batching of test images for inference. The class efficiently processes images and prepares them for the model by organizing them into batches, resizing, and normalizing as needed.\n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "1. **`__init__(self, dataframe, batch_size=16, image_size=(256, 256), normalize=False)`**\n",
    "   - **Purpose:** Initializes the dataset with key parameters for image processing and batching.\n",
    "   - **Parameters:**\n",
    "     - `dataframe`: The DataFrame containing image paths and other relevant data (e.g., `row_id`).\n",
    "     - `batch_size`: Number of images to process per batch. Defaults to 16.\n",
    "     - `image_size`: The target size (width, height) to which each image will be resized. Defaults to (256, 256) pixels.\n",
    "     - `normalize`: A flag to indicate whether images should be normalized (scaled to a range of 0 to 1).\n",
    "\n",
    "2. **`load_image(self, image_path)`**\n",
    "   - **Purpose:** Loads an image from the given file path and prepares it for inference.\n",
    "   - **Process:**\n",
    "     - If the image is in **DICOM** format (with `.dcm` extension), it is read using the `pydicom` library.\n",
    "     - For other image formats (e.g., `.png`, `.jpg`), the image is read using OpenCVâ€™s `cv2.imread()`.\n",
    "     - If the image is not found, a `FileNotFoundError` is raised.\n",
    "     - If the image is grayscale (2D), it is stacked into 3 channels to simulate an RGB image.\n",
    "     - The image is normalized (divided by 255) if `normalize` is set to `True`.\n",
    "\n",
    "3. **`__getitem__(self, index)`**\n",
    "   - **Purpose:** Retrieves a batch of images and their corresponding `row_id`s based on the current batch index.\n",
    "   - **Process:**\n",
    "     - Computes the start and end indices for the batch.\n",
    "     - Loops through the DataFrame rows corresponding to this batch.\n",
    "     - For each row, it loads the image using `load_image()`, resizes it to the specified `image_size`, and adds the image and its `row_id` to separate lists.\n",
    "     - Converts the lists of images and `row_id`s to NumPy arrays for easier batch processing in deep learning frameworks.\n",
    "   \n",
    "4. **`__len__(self)`**\n",
    "   - **Purpose:** Returns the number of batches in the dataset, calculated as the ceiling of the total number of images divided by the batch size. This ensures that any remainder images in the dataset will be included in the final batch.\n",
    "\n",
    "#### How It Works:\n",
    "\n",
    "- **Batch Processing:** The class enables batch processing by allowing the user to retrieve a batch of images and their associated `row_id`s by indexing (`__getitem__`). This is useful when working with models that expect a batch of inputs for efficient computation during inference.\n",
    "  \n",
    "- **Image Loading and Resizing:** Images are loaded, resized, and normalized as per the requirements of the model. This ensures that the images have the correct dimensions and value ranges for accurate predictions.\n",
    "\n",
    "- **Grayscale to RGB Conversion:** If an image is grayscale (2D), it is converted to a 3-channel format (simulating RGB), which is a common requirement for CNN models that are trained on RGB images.\n",
    "\n",
    "- **DICOM Handling:** Since medical images are often stored in DICOM format, the class supports loading these images using `pydicom`, ensuring compatibility with common medical imaging formats.\n",
    "\n",
    "This custom dataset class ensures that the images are processed efficiently and in a format ready for inference with a pre-trained convolutional neural network (CNN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:24.436305Z",
     "iopub.status.busy": "2024-10-06T14:58:24.435889Z",
     "iopub.status.idle": "2024-10-06T14:58:24.452214Z",
     "shell.execute_reply": "2024-10-06T14:58:24.450796Z",
     "shell.execute_reply.started": "2024-10-06T14:58:24.436261Z"
    }
   },
   "outputs": [],
   "source": [
    "class TestDataset:\n",
    "    def __init__(self, dataframe, batch_size=16, image_size=(256, 256), normalize=False):\n",
    "        self.dataframe = dataframe\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        if image_path.lower().endswith('.dcm'):\n",
    "            dicom = pydicom.dcmread(image_path, force=True)\n",
    "            image = dicom.pixel_array\n",
    "        else:\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "            if image is None:\n",
    "                raise FileNotFoundError(f\"Could not load image from {image_path}\")\n",
    "\n",
    "        # Convert image to uint8 if necessary\n",
    "        if image.dtype != np.uint8:\n",
    "            image = image.astype(np.uint8)\n",
    "\n",
    "        # If the image is grayscale, stack it to make 3 channels\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.stack([image] * 3, axis=-1)\n",
    "\n",
    "        # Normalize the image if the flag is set\n",
    "        if self.normalize:\n",
    "            image = image / 255.0  # Normalization to [0, 1]\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the starting index for this batch\n",
    "        start_index = index * self.batch_size\n",
    "        end_index = min((index + 1) * self.batch_size, len(self.dataframe))\n",
    "\n",
    "        images = []\n",
    "        row_ids = []\n",
    "\n",
    "        for i in range(start_index, end_index):\n",
    "            row = self.dataframe.iloc[i]\n",
    "            image_path = row['image_path']\n",
    "            row_id = row['row_id']\n",
    "\n",
    "            # Load and resize image\n",
    "            image = self.load_image(image_path)\n",
    "            image = cv2.resize(image, self.image_size)\n",
    "\n",
    "            images.append(image)\n",
    "            row_ids.append(row_id)\n",
    "\n",
    "        # Convert the list of images and row_ids to numpy arrays\n",
    "        images = np.array(images)\n",
    "        row_ids = np.array(row_ids)\n",
    "\n",
    "        return images, row_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of batches per epoch\n",
    "        return int(np.ceil(len(self.dataframe) / self.batch_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Instance of the TestDataset Class\n",
    "\n",
    "In this section, we create an instance of the `TestDataset` class to prepare our test dataset for inference with the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:25.664946Z",
     "iopub.status.busy": "2024-10-06T14:58:25.664432Z",
     "iopub.status.idle": "2024-10-06T14:58:25.671870Z",
     "shell.execute_reply": "2024-10-06T14:58:25.670215Z",
     "shell.execute_reply.started": "2024-10-06T14:58:25.664900Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(test_df,batch_size = 16, image_size=(256, 256), normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Pre-trained Model\n",
    "\n",
    "In this section, we load a pre-trained convolutional neural network (CNN) model that has been previously trained and saved. This model will be used to make predictions on our test dataset of lumbar spine images.\n",
    "\n",
    "#### Steps Involved:\n",
    "\n",
    "1. **Importing Required Libraries:**\n",
    "   We start by importing the necessary Keras module from TensorFlow, which provides a convenient interface for building and using deep learning models.\n",
    "\n",
    "2. **Loading the Pre-trained Model:**\n",
    "   The pre-trained model is loaded using the `load_model()` function provided by Keras. This function requires the file path of the saved model as an argument.\n",
    "\n",
    "   - **Model Path:**\n",
    "     - The specified path is where the model was saved. It is important to ensure that this path is correct and that the model file is accessible in the current environment. This path suggests that the model is stored on Kaggle's platform.\n",
    "\n",
    "3. **Purpose of Loading the Model:**\n",
    "   By loading the pre-trained model, we can utilize the knowledge that has been acquired during its training. The model is equipped to recognize patterns and features relevant to the classification of lumbar spine degenerative conditions based on the training data it has seen.\n",
    "\n",
    "4. **Next Steps:**\n",
    "   Once the model is loaded, we can proceed to perform inference on the test dataset. This will allow us to classify the images and predict their associated conditions. Evaluating the model's predictions on unseen data is crucial for assessing its performance and generalization capabilities.\n",
    "\n",
    "This step is essential for applying deep learning techniques to medical image classification tasks, enabling informed decision-making based on the predictions produced by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:27.543421Z",
     "iopub.status.busy": "2024-10-06T14:58:27.542933Z",
     "iopub.status.idle": "2024-10-06T14:58:46.675103Z",
     "shell.execute_reply": "2024-10-06T14:58:46.673794Z",
     "shell.execute_reply.started": "2024-10-06T14:58:27.543377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the saved model (make sure this is the correct path to your model)\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"/kaggle/input/cnn_us_lsdc/keras/default/1/CNN_LSDC_model.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Making Predictions on the Test Dataset\n",
    "\n",
    "In this section, we perform inference using the pre-trained model on the test dataset. The code processes the images in batches, makes predictions, and stores the results.\n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **Imports and Initialization:**\n",
    "   - We import necessary libraries such as `tqdm` for displaying progress bars and `numpy` and `pandas` for data handling.\n",
    "   - We initialize a dictionary called `results` to store the prediction results, which includes `row_id` and probabilities for each class: normal/mild, moderate, and severe.\n",
    "\n",
    "2. **Setting the Batch Size:**\n",
    "   - We define a `batch_size` variable to specify the number of images processed in each batch. In this case, it is set to 16.\n",
    "\n",
    "3. **Processing Images with a Progress Bar:**\n",
    "   - We utilize `tqdm` to create a progress bar that provides feedback on the processing status of the entire dataset.\n",
    "\n",
    "   - **Batch Iteration:**\n",
    "     - We loop through the dataset using the index to process each batch of images. For each iteration:\n",
    "       - We retrieve a batch of images and their corresponding `row_ids` from the `test_dataset`.\n",
    "       - We check if the shape of the images matches the expected dimensions of `(batch_size, 256, 256, 3)`. If not, an error is raised.\n",
    "       - We use the loaded model to predict the probabilities for each class based on the batch of images. The predictions will have a shape of `(batch_size, num_classes)`.\n",
    "\n",
    "4. **Storing Predictions:**\n",
    "   - For each image in the batch, we:\n",
    "     - Extract the probabilities from the predictions.\n",
    "     - Append the `row_id` and the respective class probabilities (normal/mild, moderate, severe) to the `results` dictionary.\n",
    "\n",
    "5. **Updating Progress Bar:**\n",
    "   - After processing each batch, we update the progress bar to reflect the number of batches processed.\n",
    "\n",
    "6. **Handling Errors:**\n",
    "   - Any errors encountered during processing are caught and printed, indicating the specific index that caused the issue.\n",
    "\n",
    "7. **Creating a DataFrame:**\n",
    "   - After processing all images, we convert the results dictionary into a pandas DataFrame called `results_df`.\n",
    "\n",
    "8. **Normalizing Probabilities:**\n",
    "   - We normalize the predicted probabilities to ensure they sum to 1 for each instance. This step is important for ensuring that the model's output can be interpreted as probabilities.\n",
    "\n",
    "9. **Saving Results:**\n",
    "   - Finally, we save the `results_df` DataFrame to a CSV file named `test_predictions.csv`, which contains the predictions for further analysis or submission.\n",
    "\n",
    "This step is essential for evaluating the model's performance on the test dataset and provides a structured format for the results that can be easily analyzed or visualized later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:58:46.677995Z",
     "iopub.status.busy": "2024-10-06T14:58:46.677207Z",
     "iopub.status.idle": "2024-10-06T14:59:16.920929Z",
     "shell.execute_reply": "2024-10-06T14:59:16.919788Z",
     "shell.execute_reply.started": "2024-10-06T14:58:46.677948Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/13 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728226727.897156     122 service.cc:145] XLA service 0x7966f000fbb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1728226727.897224     122 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
      "I0000 00:00:1728226728.637045     122 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:30<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize results storage\n",
    "results = {\n",
    "    'row_id': [],\n",
    "    'normal_mild': [],\n",
    "    'moderate': [],\n",
    "    'severe': []\n",
    "}\n",
    "\n",
    "batch_size = 16  \n",
    "\n",
    "# Use tqdm to create a progress bar for the entire dataset\n",
    "with tqdm(total=len(test_dataset), desc=\"Processing images\") as pbar:\n",
    "    # Iterate over batches of data\n",
    "    for idx in range(len(test_dataset)):\n",
    "        try:\n",
    "            # Get a batch of images and corresponding row IDs\n",
    "            images, row_ids = test_dataset[idx]\n",
    "\n",
    "            # Ensure the images have the shape (batch_size, 256, 256, 3)\n",
    "            if images.shape[1:] != (256, 256, 3):\n",
    "                raise ValueError(f\"Image batch shape is {images.shape}, expected (?, 256, 256, 3)\")\n",
    "\n",
    "            # Make predictions on the batch with verbose=0 to suppress output\n",
    "            predictions = model.predict(images, verbose=0)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "            # Append results for each image in the batch\n",
    "            for i in range(len(row_ids)):\n",
    "                probs = predictions[i]\n",
    "\n",
    "                results['row_id'].append(row_ids[i])\n",
    "                results['normal_mild'].append(probs[0])  # Class 0: Normal/Mild\n",
    "                results['moderate'].append(probs[1])     # Class 1: Moderate\n",
    "                results['severe'].append(probs[2])       # Class 2: Severe\n",
    "\n",
    "            # Update the progress bar by the batch size\n",
    "            pbar.update(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {idx}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Normalize probabilities to ensure they sum to 1\n",
    "results_df[['normal_mild', 'moderate', 'severe']] = results_df[['normal_mild', 'moderate', 'severe']].div(\n",
    "    results_df[['normal_mild', 'moderate', 'severe']].sum(axis=1), axis=0\n",
    ")\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the Results DataFrame\n",
    "\n",
    "After generating the predictions, we utilize the `info()` method on the `results_df` DataFrame to obtain a summary of its structure and contents. This step is crucial for understanding the data we have produced during the inference process.\n",
    "\n",
    "#### Key Aspects of `results_df.info()`:\n",
    "\n",
    "1. **DataFrame Summary:**\n",
    "   - The `info()` method provides a concise summary of the DataFrame, including:\n",
    "     - The number of entries (rows) and columns.\n",
    "     - The index range.\n",
    "     - The data types of each column.\n",
    "     - The count of non-null entries in each column.\n",
    "\n",
    "2. **Column Breakdown:**\n",
    "   - The DataFrame consists of the following columns:\n",
    "     - `row_id`: Unique identifier for each image, linking it to its respective condition and study.\n",
    "     - `normal_mild`: Probability score for the image being classified as normal or mild.\n",
    "     - `moderate`: Probability score indicating a moderate classification.\n",
    "     - `severe`: Probability score suggesting a severe classification.\n",
    "\n",
    "3. **Data Types:**\n",
    "   - The data types of the columns will typically include:\n",
    "     - `row_id`: Object (string type).\n",
    "     - `normal_mild`, `moderate`, `severe`: Float (probability values).\n",
    "\n",
    "4. **Non-null Counts:**\n",
    "   - The count of non-null entries helps verify that there are no missing values in the predictions, ensuring the integrity of the results.\n",
    "\n",
    "5. **Importance of the Summary:**\n",
    "   - By examining this information, we can confirm that the predictions have been successfully generated and structured correctly. It also helps identify any issues, such as missing values or incorrect data types, before proceeding with further analysis or visualizations.\n",
    "\n",
    "Overall, running `results_df.info()` is an important step to validate the output of our model and prepare for subsequent tasks like analysis, visualization, or reporting of the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:59:16.922647Z",
     "iopub.status.busy": "2024-10-06T14:59:16.922279Z",
     "iopub.status.idle": "2024-10-06T14:59:16.936125Z",
     "shell.execute_reply": "2024-10-06T14:59:16.934809Z",
     "shell.execute_reply.started": "2024-10-06T14:59:16.922608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 194 entries, 0 to 193\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   row_id       194 non-null    object \n",
      " 1   normal_mild  194 non-null    float32\n",
      " 2   moderate     194 non-null    float32\n",
      " 3   severe       194 non-null    float32\n",
      "dtypes: float32(3), object(1)\n",
      "memory usage: 3.9+ KB\n"
     ]
    }
   ],
   "source": [
    "results_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging Results per `row_id`\n",
    "\n",
    "In this section, we aggregate the prediction results for each unique `row_id` by calculating the average probabilities across the corresponding images. This process is essential for consolidating the predictions to reflect the overall assessment for each condition associated with a specific study.\n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **Grouping by `row_id`:**\n",
    "   - We use the `groupby()` method on the `results_df` DataFrame to group the data based on the `row_id`. This ensures that we consolidate predictions for each unique image identifier.\n",
    "\n",
    "   - The `as_index=False` parameter is set to maintain `row_id` as a regular column in the resulting DataFrame instead of using it as an index.\n",
    "\n",
    "2. **Calculating Mean Probabilities:**\n",
    "   - For each `row_id`, we compute the mean of the probability scores for the classes: `normal_mild`, `moderate`, and `severe`. This results in a new DataFrame called `averaged_results_df`, which contains the average probability scores for each `row_id`.\n",
    "\n",
    "3. **Normalizing Probabilities:**\n",
    "   - To ensure that the probabilities across the three classes sum to 1 for each `row_id`, we perform normalization:\n",
    "     - We calculate the sum of the probabilities for each `row_id` using `sum(axis=1)`.\n",
    "     - Each probability column (`normal_mild`, `moderate`, `severe`) is divided by the corresponding sum of probabilities. This step ensures that the predicted probabilities are properly scaled.\n",
    "\n",
    "4. **Checking for Invalid Values:**\n",
    "   - We perform a validation check to ensure that there are no negative probability values in the normalized probabilities. This is critical, as probabilities must always be non-negative and fall within the range [0, 1].\n",
    "   - If any negative probabilities are found, a `ValueError` is raised, indicating that the submission contains invalid values.\n",
    "\n",
    "5. **Importance of Averaging and Normalization:**\n",
    "   - Averaging results across multiple predictions for the same `row_id` provides a more robust estimate of the true condition.\n",
    "   - Normalizing the probabilities is essential for interpretation, as it allows for direct comparisons among the different conditions.\n",
    "\n",
    "This step is crucial for preparing the final prediction results, ensuring they are valid and ready for analysis, visualization, or submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:59:16.938808Z",
     "iopub.status.busy": "2024-10-06T14:59:16.938384Z",
     "iopub.status.idle": "2024-10-06T14:59:16.958938Z",
     "shell.execute_reply": "2024-10-06T14:59:16.957723Z",
     "shell.execute_reply.started": "2024-10-06T14:59:16.938731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Average results per row_id\n",
    "averaged_results_df = results_df.groupby('row_id', as_index=False).mean()\n",
    "\n",
    "# Normalize probabilities to ensure they sum to 1\n",
    "sum_probs = averaged_results_df[['normal_mild', 'moderate', 'severe']].sum(axis=1)\n",
    "averaged_results_df['normal_mild'] = averaged_results_df['normal_mild'] / sum_probs\n",
    "averaged_results_df['moderate'] = averaged_results_df['moderate'] / sum_probs\n",
    "averaged_results_df['severe'] = averaged_results_df['severe'] / sum_probs\n",
    "\n",
    "# Check for any invalid values\n",
    "if (averaged_results_df[['normal_mild', 'moderate', 'severe']] < 0).any().any():\n",
    "    raise ValueError(\"Found negative probabilities in submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification of Probability Normalization\n",
    "\n",
    "In this step, we verify that the normalized probabilities for each condition sum to 1 for every `row_id` in the `averaged_results_df` DataFrame. This check ensures that the normalization process was successful and that the probability values are valid.\n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **Calculating the Sum of Probabilities:**\n",
    "   - We create a new column called `sum_check` in the `averaged_results_df` DataFrame. This column will hold the sum of the probabilities for the three conditions: `normal_mild`, `moderate`, and `severe`.\n",
    "   - The sum is calculated using the `sum(axis=1)` method, which sums the values across the specified columns for each row.\n",
    "\n",
    "2. **Rounding the Sum Values:**\n",
    "   - To enhance readability and ensure consistency, we apply the `round(x, 2)` function to round the sum to two decimal places. This rounding helps in clearly observing the results without excessive precision that may not be meaningful in the context of probabilities.\n",
    "\n",
    "3. **Displaying the Normalization Check:**\n",
    "   - We print a confirmation message \"Normalization Check:\" to indicate that we are about to display the results of the verification process.\n",
    "   - The `head()` method is used to show the first few entries of the `row_id` along with their corresponding `sum_check` values. This display allows us to quickly assess whether the normalization was successful across several samples.\n",
    "\n",
    "4. **Importance of the Normalization Check:**\n",
    "   - Ensuring that the sum of probabilities equals 1 is crucial for validating the model's outputs. Probabilities must adhere to the properties of a probability distribution, where the sum of all possible outcomes should equal 1.\n",
    "   - This check serves as a final validation step before proceeding to utilize or submit the results, providing confidence in the integrity of the data.\n",
    "\n",
    "By performing this verification, we confirm that our predictions are reliable and ready for further analysis or reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:59:16.960881Z",
     "iopub.status.busy": "2024-10-06T14:59:16.960434Z",
     "iopub.status.idle": "2024-10-06T14:59:16.984677Z",
     "shell.execute_reply": "2024-10-06T14:59:16.983402Z",
     "shell.execute_reply.started": "2024-10-06T14:59:16.960838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization Check:\n",
      "                                             row_id  sum_check\n",
      "0    44036939_left_neural_foraminal_narrowing_l1_l2        1.0\n",
      "1    44036939_left_neural_foraminal_narrowing_l2_l3        1.0\n",
      "2    44036939_left_neural_foraminal_narrowing_l3_l4        1.0\n",
      "3    44036939_left_neural_foraminal_narrowing_l4_l5        1.0\n",
      "4    44036939_left_neural_foraminal_narrowing_l5_s1        1.0\n",
      "5         44036939_left_subarticular_stenosis_l1_l2        1.0\n",
      "6         44036939_left_subarticular_stenosis_l2_l3        1.0\n",
      "7         44036939_left_subarticular_stenosis_l3_l4        1.0\n",
      "8         44036939_left_subarticular_stenosis_l4_l5        1.0\n",
      "9         44036939_left_subarticular_stenosis_l5_s1        1.0\n",
      "10  44036939_right_neural_foraminal_narrowing_l1_l2        1.0\n",
      "11  44036939_right_neural_foraminal_narrowing_l2_l3        1.0\n",
      "12  44036939_right_neural_foraminal_narrowing_l3_l4        1.0\n",
      "13  44036939_right_neural_foraminal_narrowing_l4_l5        1.0\n",
      "14  44036939_right_neural_foraminal_narrowing_l5_s1        1.0\n",
      "15       44036939_right_subarticular_stenosis_l1_l2        1.0\n",
      "16       44036939_right_subarticular_stenosis_l2_l3        1.0\n",
      "17       44036939_right_subarticular_stenosis_l3_l4        1.0\n",
      "18       44036939_right_subarticular_stenosis_l4_l5        1.0\n",
      "19       44036939_right_subarticular_stenosis_l5_s1        1.0\n",
      "20             44036939_spinal_canal_stenosis_l1_l2        1.0\n",
      "21             44036939_spinal_canal_stenosis_l2_l3        1.0\n",
      "22             44036939_spinal_canal_stenosis_l3_l4        1.0\n",
      "23             44036939_spinal_canal_stenosis_l4_l5        1.0\n",
      "24             44036939_spinal_canal_stenosis_l5_s1        1.0\n"
     ]
    }
   ],
   "source": [
    "# Verify that the sum of probabilities is 1 for each row\n",
    "averaged_results_df['sum_check'] = averaged_results_df[['normal_mild', 'moderate', 'severe']].sum(axis=1).apply(lambda x: round(x, 2))\n",
    "print(\"Normalization Check:\")\n",
    "print(averaged_results_df[['row_id', 'sum_check']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights on Normalization Check Results\n",
    "\n",
    "1. **Successful Normalization:**\n",
    "   - The `sum_check` column indicates that the sum of the predicted probabilities for each `row_id` equals **1.0** for all entries shown. This confirms that the normalization process was successful and that the probability values are valid.\n",
    "\n",
    "2. **Consistency Across Conditions:**\n",
    "   - Each `row_id` corresponds to a specific spinal condition (e.g., left neural foraminal narrowing at various lumbar levels). The consistency of the sum being **1.0** across different conditions indicates that the model is effectively outputting probabilities that reflect the relative likelihood of each classification (normal/mild, moderate, severe).\n",
    "\n",
    "3. **Data Integrity:**\n",
    "   - The fact that all rows show a sum of exactly **1.0** is a positive sign of data integrity. It suggests that the predictions generated by the model are reliable and adhere to the mathematical properties expected of probabilities.\n",
    "\n",
    "4. **Interpretation of Results:**\n",
    "   - Since the probabilities are normalized, they can be directly interpreted as the modelâ€™s confidence levels for each condition. For instance, if the probabilities were `[0.7, 0.2, 0.1]` for a specific `row_id`, this would indicate a strong confidence that the condition is classified as normal/mild.\n",
    "\n",
    "5. **Validation for Clinical Use:**\n",
    "   - Normalization is especially critical in medical applications. It ensures that the model's predictions can be trusted and used for decision-making processes. Clinicians can rely on the output as a basis for further assessments or interventions.\n",
    "\n",
    "6. **Next Steps:**\n",
    "   - With the normalization successfully verified, you can proceed to utilize these predictions for clinical reporting or submission in competitions. It would also be beneficial to visualize some of the results to better understand the model's performance and potentially identify areas for improvement.\n",
    "\n",
    "### Conclusion\n",
    "Overall, the normalization check demonstrates that the model's predictions are mathematically sound and ready for practical application. This step is essential in ensuring the robustness of the inference process in a real-world setting, particularly in medical imaging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Submission DataFrame\n",
    "\n",
    "In this section, we create a new DataFrame specifically designed for submission purposes. This DataFrame will contain the final average probability results for each unique `row_id`, allowing for straightforward analysis or submission.\n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **Creating the Submission DataFrame:**\n",
    "   - We define `submission_df` by selecting specific columns from the `averaged_results_df`. The columns included are:\n",
    "     - `row_id`: The unique identifier for each image, linking it to its respective condition and study.\n",
    "     - `normal_mild`: The averaged probability score indicating the likelihood that the condition is classified as normal or mild.\n",
    "     - `moderate`: The averaged probability score representing a moderate classification.\n",
    "     - `severe`: The averaged probability score suggesting a severe classification.\n",
    "\n",
    "   This selection ensures that only the relevant data for submission is retained in the new DataFrame.\n",
    "\n",
    "2. **Structure of the Submission DataFrame:**\n",
    "   - The resulting `submission_df` will have the following columns:\n",
    "     - **row_id**: A string that uniquely identifies each row.\n",
    "     - **normal_mild**: A float representing the predicted probability of the normal/mild condition.\n",
    "     - **moderate**: A float representing the predicted probability of the moderate condition.\n",
    "     - **severe**: A float representing the predicted probability of the severe condition.\n",
    "\n",
    "3. **Viewing the Submission DataFrame:**\n",
    "   - By calling `submission_df`, we can preview the structure and contents of the DataFrame. This step is essential for verifying that the data has been correctly organized and is ready for further processing, such as exporting to a CSV file for submission or reporting.\n",
    "\n",
    "4. **Importance of the Submission DataFrame:**\n",
    "   - The `submission_df` serves as the final output of our inference process. It consolidates all the predictions into a format that can be easily interpreted, shared, or submitted to relevant stakeholders or platforms.\n",
    "   - Ensuring that the data is structured correctly is crucial for effective communication of the modelâ€™s findings and performance on the test dataset.\n",
    "\n",
    "This preparation step is vital for the completion of the project, marking the transition from model inference to result dissemination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:59:21.141969Z",
     "iopub.status.busy": "2024-10-06T14:59:21.141501Z",
     "iopub.status.idle": "2024-10-06T14:59:21.161316Z",
     "shell.execute_reply": "2024-10-06T14:59:21.159812Z",
     "shell.execute_reply.started": "2024-10-06T14:59:21.141923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>normal_mild</th>\n",
       "      <th>moderate</th>\n",
       "      <th>severe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l1_l2</td>\n",
       "      <td>0.379058</td>\n",
       "      <td>0.290456</td>\n",
       "      <td>0.330486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l2_l3</td>\n",
       "      <td>0.386446</td>\n",
       "      <td>0.305325</td>\n",
       "      <td>0.308228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l3_l4</td>\n",
       "      <td>0.409427</td>\n",
       "      <td>0.312947</td>\n",
       "      <td>0.277626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l4_l5</td>\n",
       "      <td>0.375377</td>\n",
       "      <td>0.284160</td>\n",
       "      <td>0.340463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l5_s1</td>\n",
       "      <td>0.384381</td>\n",
       "      <td>0.292800</td>\n",
       "      <td>0.322819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l1_l2</td>\n",
       "      <td>0.412525</td>\n",
       "      <td>0.373115</td>\n",
       "      <td>0.214359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l2_l3</td>\n",
       "      <td>0.413570</td>\n",
       "      <td>0.368270</td>\n",
       "      <td>0.218160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l3_l4</td>\n",
       "      <td>0.374798</td>\n",
       "      <td>0.350470</td>\n",
       "      <td>0.274732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l4_l5</td>\n",
       "      <td>0.345216</td>\n",
       "      <td>0.443290</td>\n",
       "      <td>0.211494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l5_s1</td>\n",
       "      <td>0.426723</td>\n",
       "      <td>0.361181</td>\n",
       "      <td>0.212096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l1_l2</td>\n",
       "      <td>0.379058</td>\n",
       "      <td>0.290456</td>\n",
       "      <td>0.330486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l2_l3</td>\n",
       "      <td>0.386446</td>\n",
       "      <td>0.305325</td>\n",
       "      <td>0.308228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l3_l4</td>\n",
       "      <td>0.409427</td>\n",
       "      <td>0.312947</td>\n",
       "      <td>0.277626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l4_l5</td>\n",
       "      <td>0.375377</td>\n",
       "      <td>0.284160</td>\n",
       "      <td>0.340463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l5_s1</td>\n",
       "      <td>0.384381</td>\n",
       "      <td>0.292800</td>\n",
       "      <td>0.322819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l1_l2</td>\n",
       "      <td>0.345216</td>\n",
       "      <td>0.443290</td>\n",
       "      <td>0.211494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l2_l3</td>\n",
       "      <td>0.426723</td>\n",
       "      <td>0.361181</td>\n",
       "      <td>0.212096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l3_l4</td>\n",
       "      <td>0.412525</td>\n",
       "      <td>0.373115</td>\n",
       "      <td>0.214359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l4_l5</td>\n",
       "      <td>0.413570</td>\n",
       "      <td>0.368270</td>\n",
       "      <td>0.218160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l5_s1</td>\n",
       "      <td>0.374798</td>\n",
       "      <td>0.350470</td>\n",
       "      <td>0.274732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l1_l2</td>\n",
       "      <td>0.364865</td>\n",
       "      <td>0.275967</td>\n",
       "      <td>0.359169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l2_l3</td>\n",
       "      <td>0.374605</td>\n",
       "      <td>0.283533</td>\n",
       "      <td>0.341861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l3_l4</td>\n",
       "      <td>0.375199</td>\n",
       "      <td>0.284001</td>\n",
       "      <td>0.340800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l4_l5</td>\n",
       "      <td>0.364865</td>\n",
       "      <td>0.275967</td>\n",
       "      <td>0.359169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l5_s1</td>\n",
       "      <td>0.364865</td>\n",
       "      <td>0.275967</td>\n",
       "      <td>0.359169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             row_id  normal_mild  moderate  \\\n",
       "0    44036939_left_neural_foraminal_narrowing_l1_l2     0.379058  0.290456   \n",
       "1    44036939_left_neural_foraminal_narrowing_l2_l3     0.386446  0.305325   \n",
       "2    44036939_left_neural_foraminal_narrowing_l3_l4     0.409427  0.312947   \n",
       "3    44036939_left_neural_foraminal_narrowing_l4_l5     0.375377  0.284160   \n",
       "4    44036939_left_neural_foraminal_narrowing_l5_s1     0.384381  0.292800   \n",
       "5         44036939_left_subarticular_stenosis_l1_l2     0.412525  0.373115   \n",
       "6         44036939_left_subarticular_stenosis_l2_l3     0.413570  0.368270   \n",
       "7         44036939_left_subarticular_stenosis_l3_l4     0.374798  0.350470   \n",
       "8         44036939_left_subarticular_stenosis_l4_l5     0.345216  0.443290   \n",
       "9         44036939_left_subarticular_stenosis_l5_s1     0.426723  0.361181   \n",
       "10  44036939_right_neural_foraminal_narrowing_l1_l2     0.379058  0.290456   \n",
       "11  44036939_right_neural_foraminal_narrowing_l2_l3     0.386446  0.305325   \n",
       "12  44036939_right_neural_foraminal_narrowing_l3_l4     0.409427  0.312947   \n",
       "13  44036939_right_neural_foraminal_narrowing_l4_l5     0.375377  0.284160   \n",
       "14  44036939_right_neural_foraminal_narrowing_l5_s1     0.384381  0.292800   \n",
       "15       44036939_right_subarticular_stenosis_l1_l2     0.345216  0.443290   \n",
       "16       44036939_right_subarticular_stenosis_l2_l3     0.426723  0.361181   \n",
       "17       44036939_right_subarticular_stenosis_l3_l4     0.412525  0.373115   \n",
       "18       44036939_right_subarticular_stenosis_l4_l5     0.413570  0.368270   \n",
       "19       44036939_right_subarticular_stenosis_l5_s1     0.374798  0.350470   \n",
       "20             44036939_spinal_canal_stenosis_l1_l2     0.364865  0.275967   \n",
       "21             44036939_spinal_canal_stenosis_l2_l3     0.374605  0.283533   \n",
       "22             44036939_spinal_canal_stenosis_l3_l4     0.375199  0.284001   \n",
       "23             44036939_spinal_canal_stenosis_l4_l5     0.364865  0.275967   \n",
       "24             44036939_spinal_canal_stenosis_l5_s1     0.364865  0.275967   \n",
       "\n",
       "      severe  \n",
       "0   0.330486  \n",
       "1   0.308228  \n",
       "2   0.277626  \n",
       "3   0.340463  \n",
       "4   0.322819  \n",
       "5   0.214359  \n",
       "6   0.218160  \n",
       "7   0.274732  \n",
       "8   0.211494  \n",
       "9   0.212096  \n",
       "10  0.330486  \n",
       "11  0.308228  \n",
       "12  0.277626  \n",
       "13  0.340463  \n",
       "14  0.322819  \n",
       "15  0.211494  \n",
       "16  0.212096  \n",
       "17  0.214359  \n",
       "18  0.218160  \n",
       "19  0.274732  \n",
       "20  0.359169  \n",
       "21  0.341861  \n",
       "22  0.340800  \n",
       "23  0.359169  \n",
       "24  0.359169  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = averaged_results_df[['row_id', 'normal_mild', 'moderate', 'severe']]\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Submission File\n",
    "\n",
    "In this final step, we save the prepared submission DataFrame (`submission_df`) to a CSV file. This file can be used for further analysis, sharing, or submission to relevant platforms.\n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **Exporting the DataFrame to CSV:**\n",
    "   - We utilize the `to_csv()` method from the pandas library to export `submission_df` to a CSV file.\n",
    "   - The argument `index=False` is specified to prevent pandas from writing row indices to the CSV file. This is important because we want to keep the file clean and ensure it only contains the relevant data columns.\n",
    "\n",
    "2. **Naming the Submission File:**\n",
    "   - The submission file is named `submission.csv`, and this name is indicated in the print statement for clarity. The CSV file will contain the `row_id` and the corresponding averaged probability scores for the conditions: normal/mild, moderate, and severe.\n",
    "\n",
    "3. **Outputting a Confirmation Message:**\n",
    "   - A confirmation message is printed to the console to inform the user that the submission file has been successfully saved. This provides assurance that the data has been correctly exported.\n",
    "\n",
    "4. **Saving to Kaggle Working Directory:**\n",
    "   - The command `submission_df.to_csv('/kaggle/working/submission.csv', index=False)` ensures that the file is saved to the Kaggle working directory, making it accessible for download or further use within the Kaggle environment.\n",
    "\n",
    "5. **Importance of Saving the Submission File:**\n",
    "   - Saving the predictions in a structured format (CSV) is essential for effective communication of results.\n",
    "   - The submission file can be submitted to competitions or assessments where model performance needs to be evaluated.\n",
    "   - This step marks the conclusion of the data processing and inference pipeline, transitioning from analysis to actionable outcomes.\n",
    "\n",
    "By completing this step, we ensure that our results are preserved and ready for future reference or evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:59:22.887166Z",
     "iopub.status.busy": "2024-10-06T14:59:22.886617Z",
     "iopub.status.idle": "2024-10-06T14:59:22.898431Z",
     "shell.execute_reply": "2024-10-06T14:59:22.897068Z",
     "shell.execute_reply.started": "2024-10-06T14:59:22.887114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as 'submission.csv'.\n"
     ]
    }
   ],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)\n",
    "# Save the submission file\n",
    "submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "print(\"Submission file saved as 'submission.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we successfully implemented the inference process for the RSNA Lumbar Spine Degenerative Classification task using a pre-trained Convolutional Neural Network (CNN). The key steps undertaken include:\n",
    "\n",
    "1. **Data Preparation:** \n",
    "   - We loaded and processed the test dataset, ensuring that all images were correctly accessed and organized.\n",
    "\n",
    "2. **Model Loading and Predictions:** \n",
    "   - We utilized a pre-trained CNN model to make predictions on the test dataset. The model output was carefully handled to ensure accurate results.\n",
    "\n",
    "3. **Results Aggregation and Normalization:** \n",
    "   - We computed the average probabilities for each condition across multiple images corresponding to each `row_id`, ensuring that the probabilities were normalized to sum to 1.\n",
    "\n",
    "4. **Verification:** \n",
    "   - A final check confirmed that the normalized probabilities adhered to the fundamental properties of probability distributions.\n",
    "\n",
    "5. **Submission Preparation:**\n",
    "   - The processed results were saved in a structured CSV format, ready for submission or further analysis.\n",
    "\n",
    "Through these steps, we have built a robust inference pipeline that can be adapted for similar classification tasks in medical imaging. The results generated can be valuable for clinical evaluations and decision-making regarding lumbar spine conditions. Future work may focus on improving model accuracy through fine-tuning and exploring additional data augmentation techniques.\n",
    "\n",
    "Thank you for reviewing this notebook, and I look forward to any questions or feedback!\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8561470,
     "sourceId": 71549,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 132238,
     "modelInstanceId": 107901,
     "sourceId": 128122,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
